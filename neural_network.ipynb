{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n",
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_18vdbq-ErL3",
        "outputId": "a9ea0108-20e8-4284-b41a-21408d648468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.11.17)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Gi5_bjCTTl",
        "outputId": "2ef00912-1ed0-4ebe-bfb2-fd613de5427c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/397.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m266.2/397.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk as nltk\n",
        "import re\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import emoji\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from nltk import pos_tag\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQUvXWV0zRIu",
        "outputId": "69ff924d-59e7-4d3f-abf1-a0a7afaf5280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/train.xlsx\")\n",
        "data = pd.DataFrame(data,columns=['review_description', 'rating'])\n",
        "data.columns = ['review_description','rating']\n",
        "\n",
        "# Extract feature & label\n",
        "X = data['review_description']\n",
        "Y = data['rating']\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NkFsfgA3s-E",
        "outputId": "fb27a953-1282-4564-cfe6-ac8376975a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        شركه زباله و سواقين بتبرشم و مفيش حتي رقم للشك...\n",
            "1        خدمة الدفع عن طريق الكي نت توقفت عندي اصبح فقط...\n",
            "2        تطبيق غبي و جاري حذفه ، عاملين اكواد خصم و لما...\n",
            "3        فعلا تطبيق ممتاز بس لو فى امكانية يتيح لمستخدم...\n",
            "4        سيء جدا ، اسعار رسوم التوصيل لا تمت للواقع ب ص...\n",
            "                               ...                        \n",
            "32031    التطبيق اصبح سيء للغايه نقوم بطلب لا يتم وصول ...\n",
            "32032                                           y love you\n",
            "32033        الباقه بتخلص وبشحن مرتين باقه اضافيه ١٠٠ جنيه\n",
            "32034    تطبيق فاشل وصلني الطلب ناقص ومش ينفع اعمل حاجة...\n",
            "32035                                      ليش ما يفتح معي\n",
            "Name: review_description, Length: 32036, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import emoji\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Translate text to Arabic\n",
        "\n",
        "\n",
        "    punctuation_re = '[?؟!٪,،@#$%&*€+-£_~\\“̯/=><.\\۰):؛}{÷%(\"\\'ًٌٍَُِّْ٠-٩]'\n",
        "\n",
        "    emoji_re = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U0001f926-\\U0001f937\"\n",
        "            u\"\\U00010000-\\U0010ffff\"\n",
        "            u\"\\u2640-\\u2642\"\n",
        "            u\"\\u2600-\\u2B55\"\n",
        "            u\"\\u200d\"\n",
        "            u\"\\u23cf\"\n",
        "            u\"\\u23e9\"\n",
        "            u\"\\u231a\"\n",
        "            u\"\\ufe0f\"  # dingbats\n",
        "            u\"\\u3030\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    stop_words = set(stopwords.words(\"arabic\"))\n",
        "    stemmer = SnowballStemmer('arabic')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Remove punctuation\n",
        "    no_punc = re.sub(punctuation_re, ' ', str(text))\n",
        "    emojis = {\n",
        "        \"🙂\":\"يبتسم\",\n",
        "        \"😂\":\"يضحك\",\n",
        "        \"💔\":\"قلب حزين\",\n",
        "        \"🙂\":\"يبتسم\",\n",
        "        \"❤\":\"حب\",\n",
        "        \"❤\":\"حب\",\n",
        "        \"😍\":\"حب\",\n",
        "        \"😭\":\"يبكي\",\n",
        "        \"😢\":\"حزن\",\n",
        "        \"😔\":\"حزن\",\n",
        "        \"♥\":\"حب\",\n",
        "        \"💜\":\"حب\",\n",
        "        \"😅\":\"يضحك\",\n",
        "        \"🙁\":\"حزين\",\n",
        "        \"💕\":\"حب\",\n",
        "        \"💙\":\"حب\",\n",
        "        \"😞\":\"حزين\",\n",
        "        \"😊\":\"سعادة\",\n",
        "        \"👏\":\"يصفق\",\n",
        "        \"👌\":\"احسنت\",\n",
        "        \"😴\":\"ينام\",\n",
        "        \"😀\":\"يضحك\",\n",
        "        \"😌\":\"حزين\",\n",
        "        \"🌹\":\"وردة\",\n",
        "        \"🙈\":\"حب\",\n",
        "        \"😄\":\"يضحك\",\n",
        "        \"😐\":\"محايد\",\n",
        "        \"✌\":\"منتصر\",\n",
        "        \"✨\":\"نجمه\",\n",
        "        \"🤔\":\"تفكير\",\n",
        "        \"😏\":\"يستهزء\",\n",
        "        \"😒\":\"يستهزء\",\n",
        "        \"🙄\":\"ملل\",\n",
        "        \"😕\":\"عصبية\",\n",
        "        \"😃\":\"يضحك\",\n",
        "        \"🌸\":\"وردة\",\n",
        "        \"😓\":\"حزن\",\n",
        "        \"💞\":\"حب\",\n",
        "        \"💗\":\"حب\",\n",
        "        \"😑\":\"منزعج\",\n",
        "        \"💭\":\"تفكير\",\n",
        "        \"😎\":\"ثقة\",\n",
        "        \"💛\":\"حب\",\n",
        "        \"😩\":\"حزين\",\n",
        "        \"💪\":\"عضلات\",\n",
        "        \"👍\":\"موافق\",\n",
        "        \"🙏🏻\":\"رجاء طلب\",\n",
        "        \"😳\":\"مصدوم\",\n",
        "        \"👏🏼\":\"تصفيق\",\n",
        "        \"🎶\":\"موسيقي\",\n",
        "        \"🌚\":\"صمت\",\n",
        "        \"💚\":\"حب\",\n",
        "        \"🙏\":\"رجاء طلب\",\n",
        "        \"💘\":\"حب\",\n",
        "        \"🍃\":\"سلام\",\n",
        "        \"☺\":\"يضحك\",\n",
        "        \"🐸\":\"ضفدع\",\n",
        "        \"😶\":\"مصدوم\",\n",
        "        \"✌\":\"مرح\",\n",
        "        \"✋🏻\":\"توقف\",\n",
        "        \"😉\":\"غمزة\",\n",
        "        \"🌷\":\"حب\",\n",
        "        \"🙃\":\"مبتسم\",\n",
        "        \"😫\":\"حزين\",\n",
        "        \"😨\":\"مصدوم\",\n",
        "        \"🎼 \":\"موسيقي\",\n",
        "        \"🍁\":\"مرح\",\n",
        "        \"🍂\":\"مرح\",\n",
        "        \"💟\":\"حب\",\n",
        "        \"😪\":\"حزن\",\n",
        "        \"😆\":\"يضحك\",\n",
        "        \"😣\":\"استياء\",\n",
        "        \"☺\":\"حب\",\n",
        "        \"😱\":\"كارثة\",\n",
        "        \"😁\":\"يضحك\",\n",
        "        \"😖\":\"استياء\",\n",
        "        \"🏃🏼\":\"يجري\",\n",
        "        \"😡\":\"غضب\",\n",
        "        \"🚶\":\"يسير\",\n",
        "        \"🤕\":\"مرض\",\n",
        "        \"‼\":\"تعجب\",\n",
        "        \"🕊\":\"طائر\",\n",
        "        \"👌🏻\":\"احسنت\",\n",
        "        \"❣\":\"حب\",\n",
        "        \"🙊\":\"مصدوم\",\n",
        "        \"💃\":\"سعادة مرح\",\n",
        "        \"💃🏼\":\"سعادة مرح\",\n",
        "        \"😜\":\"مرح\",\n",
        "        \"👊\":\"ضربة\",\n",
        "        \"😟\":\"استياء\",\n",
        "        \"💖\":\"حب\",\n",
        "        \"😥\":\"حزن\",\n",
        "        \"🎻\":\"موسيقي\",\n",
        "        \"✒\":\"يكتب\",\n",
        "        \"🚶🏻\":\"يسير\",\n",
        "        \"💎\":\"الماظ\",\n",
        "        \"😷\":\"وباء مرض\",\n",
        "        \"☝\":\"واحد\",\n",
        "        \"🚬\":\"تدخين\",\n",
        "        \"💐\" : \"ورد\",\n",
        "        \"🌞\" : \"شمس\",\n",
        "        \"👆\" : \"الاول\",\n",
        "        \"⚠\" :\"تحذير\",\n",
        "        \"🤗\" : \"احتواء\",\n",
        "        \"✖\": \"غلط\",\n",
        "        \"📍\"  : \"مكان\",\n",
        "        \"👸\" : \"ملكه\",\n",
        "        \"👑\" : \"تاج\",\n",
        "        \"✔\" : \"صح\",\n",
        "        \"💌\": \"قلب\",\n",
        "        \"😲\" : \"مندهش\",\n",
        "        \"💦\": \"ماء\",\n",
        "        \"🚫\" : \"خطا\",\n",
        "        \"👏🏻\" : \"برافو\",\n",
        "        \"🏊\" :\"يسبح\",\n",
        "        \"👍🏻\": \"تمام\",\n",
        "        \"⭕\" :\"دائره كبيره\",\n",
        "        \"🎷\" : \"ساكسفون\",\n",
        "        \"👋\": \"تلويح باليد\",\n",
        "        \"✌🏼\": \"علامه النصر\",\n",
        "        \"🌝\":\"مبتسم\",\n",
        "        \"➿\"  : \"عقده مزدوجه\",\n",
        "        \"💪🏼\" : \"قوي\",\n",
        "        \"📩\":  \"تواصل معي\",\n",
        "        \"☕\": \"قهوه\",\n",
        "        \"😧\" : \"قلق و صدمة\",\n",
        "        \"🗨\": \"رسالة\",\n",
        "        \"❗\" :\"تعجب\",\n",
        "        \"🙆🏻\": \"اشاره موافقه\",\n",
        "        \"👯\" :\"اخوات\",\n",
        "        \"©\" :  \"رمز\",\n",
        "        \"👵🏽\" :\"سيده عجوزه\",\n",
        "        \"🐣\": \"كتكوت\",\n",
        "        \"🙌\": \"تشجيع\",\n",
        "        \"🙇\": \"شخص ينحني\",\n",
        "        \"👐🏽\":\"ايدي مفتوحه\",\n",
        "        \"👌🏽\": \"بالظبط\",\n",
        "        \"⁉\" : \"استنكار\",\n",
        "        \"⚽\": \"كوره\",\n",
        "        \"🕶\" :\"حب\",\n",
        "        \"🎈\" :\"بالون\",\n",
        "        \"🎀\":    \"ورده\",\n",
        "        \"💵\":  \"فلوس\",\n",
        "        \"😋\":  \"جائع\",\n",
        "        \"😛\":  \"يغيظ\",\n",
        "        \"😠\":  \"غاضب\",\n",
        "        \"✍🏻\":  \"يكتب\",\n",
        "        \"🌾\":  \"ارز\",\n",
        "        \"👣\":  \"اثر قدمين\",\n",
        "        \"❌\":\"رفض\",\n",
        "        \"🍟\":\"طعام\",\n",
        "        \"👬\":\"صداقة\",\n",
        "        \"🐰\":\"ارنب\",\n",
        "        \"☂\":\"مطر\",\n",
        "        \"⚜\":\"مملكة فرنسا\",\n",
        "        \"🐑\":\"خروف\",\n",
        "        \"🗣\":\"صوت مرتفع\",\n",
        "        \"👌🏼\":\"احسنت\",\n",
        "        \"☘\":\"مرح\",\n",
        "        \"😮\":\"صدمة\",\n",
        "        \"😦\":\"قلق\",\n",
        "        \"⭕\":\"الحق\",\n",
        "        \"✏\":\"قلم\",\n",
        "        \"ℹ\":\"معلومات\",\n",
        "        \"🙍🏻\":\"رفض\",\n",
        "        \"⚪\":\"نضارة نقاء\",\n",
        "        \"🐤\":\"حزن\",\n",
        "        \"💫\":\"مرح\",\n",
        "        \"💝\":\"حب\",\n",
        "        \"🍔\":\"طعام\",\n",
        "        \"❤\":\"حب\",\n",
        "        \"✈\":\"سفر\",\n",
        "        \"🏃🏻‍♀\":\"يسير\",\n",
        "        \"🍳\":\"ذكر\",\n",
        "        \"🎤\":\"مايك غناء\",\n",
        "        \"🎾\":\"كره\",\n",
        "        \"🐔\":\"دجاجة\",\n",
        "        \"🙋\":\"سؤال\",\n",
        "        \"📮\":\"بحر\",\n",
        "        \"💉\":\"دواء\",\n",
        "        \"🙏🏼\":\"رجاء طلب\",\n",
        "        \"💂🏿 \":\"حارس\",\n",
        "        \"🎬\":\"سينما\",\n",
        "        \"♦\":\"مرح\",\n",
        "        \"💡\":\"قكرة\",\n",
        "        \"‼\":\"تعجب\",\n",
        "        \"👼\":\"طفل\",\n",
        "        \"🔑\":\"مفتاح\",\n",
        "        \"♥\":\"حب\",\n",
        "        \"🕋\":\"كعبة\",\n",
        "        \"🐓\":\"دجاجة\",\n",
        "        \"💩\":\"معترض\",\n",
        "        \"👽\":\"فضائي\",\n",
        "        \"☔\":\"مطر\",\n",
        "        \"🍷\":\"عصير\",\n",
        "        \"🌟\":\"نجمة\",\n",
        "        \"☁\":\"سحب\",\n",
        "        \"👃\":\"معترض\",\n",
        "        \"🌺\":\"مرح\",\n",
        "        \"🔪\":\"سكينة\",\n",
        "        \"♨\":\"سخونية\",\n",
        "        \"👊🏼\":\"ضرب\",\n",
        "        \"✏\":\"قلم\",\n",
        "        \"🚶🏾‍♀\":\"يسير\",\n",
        "        \"👊\":\"ضربة\",\n",
        "        \"◾\":\"وقف\",\n",
        "        \"😚\":\"حب\",\n",
        "        \"🔸\":\"مرح\",\n",
        "        \"👎🏻\":\"لا يعجبني\",\n",
        "        \"👊🏽\":\"ضربة\",\n",
        "        \"😙\":\"حب\",\n",
        "        \"🎥\":\"تصوير\",\n",
        "        \"👉\":\"جذب انتباه\",\n",
        "        \"👏🏽\":\"يصفق\",\n",
        "        \"💪🏻\":\"عضلات\",\n",
        "        \"🏴\":\"اسود\",\n",
        "        \"🔥\":\"حريق\",\n",
        "        \"😬\":\"عدم الراحة\",\n",
        "        \"👊🏿\":\"يضرب\",\n",
        "        \"🌿\":\"ورقه شجره\",\n",
        "        \"✋🏼\":\"كف ايد\",\n",
        "        \"👐\":\"ايدي مفتوحه\",\n",
        "        \"☠\":\"وجه مرعب\",\n",
        "        \"🎉\":\"يهنئ\",\n",
        "        \"🔕\" :\"صامت\",\n",
        "        \"😿\":\"وجه حزين\",\n",
        "        \"☹\":\"وجه يائس\",\n",
        "        \"😘\" :\"حب\",\n",
        "        \"😰\" :\"خوف و حزن\",\n",
        "        \"🌼\":\"ورده\",\n",
        "        \"💋\":  \"بوسه\",\n",
        "        \"👇\":\"لاسفل\",\n",
        "        \"❣\":\"حب\",\n",
        "        \"🎧\":\"سماعات\",\n",
        "        \"📝\":\"يكتب\",\n",
        "        \"😇\":\"دايخ\",\n",
        "        \"😈\":\"رعب\",\n",
        "        \"🏃\":\"يجري\",\n",
        "        \"✌🏻\":\"علامه النصر\",\n",
        "        \"🔫\":\"يضرب\",\n",
        "        \"❗\":\"تعجب\",\n",
        "        \"👎\":\"غير موافق\",\n",
        "        \"🔐\":\"قفل\",\n",
        "        \"👈\":\"لليمين\",\n",
        "        \"™\":\"رمز\",\n",
        "        \"🚶🏽\":\"يتمشي\",\n",
        "        \"😯\":\"متفاجأ\",\n",
        "        \"✊\":\"يد مغلقه\",\n",
        "        \"😻\":\"اعجاب\",\n",
        "        \"🙉\" :\"قرد\",\n",
        "        \"👧\":\"طفله صغيره\",\n",
        "        \"🔴\":\"دائره حمراء\",\n",
        "        \"💪🏽\":\"قوه\",\n",
        "        \"💤\":\"ينام\",\n",
        "        \"👀\":\"ينظر\",\n",
        "        \"✍🏻\":\"يكتب\",\n",
        "        \"❄\":\"تلج\",\n",
        "        \"💀\":\"رعب\",\n",
        "        \"😤\":\"وجه عابس\",\n",
        "        \"🖋\":\"قلم\",\n",
        "        \"🎩\":\"كاب\",\n",
        "        \"☕\":\"قهوه\",\n",
        "        \"😹\":\"ضحك\",\n",
        "        \"💓\":\"حب\",\n",
        "        \"☄ \":\"نار\",\n",
        "        \"👻\":\"رعب\",\n",
        "        \"❎\":\"خطء\",\n",
        "        \"🤮\":\"حزن\",\n",
        "        '🏻':\"احمر\"\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    emoticons_to_emoji = {\n",
        "        \":)\" : \"🙂\",\n",
        "        \":(\" : \"🙁\",\n",
        "        \"xD\" : \"😆\",\n",
        "        \":=(\": \"😭\",\n",
        "        \":'(\": \"😢\",\n",
        "        \":'‑(\": \"😢\",\n",
        "        \"XD\" : \"😂\",\n",
        "        \":D\" : \"🙂\",\n",
        "        \"♬\" : \"موسيقي\",\n",
        "        \"♡\" : \"❤\",\n",
        "        \"☻\"  : \"🙂\",\n",
        "        }\n",
        "    def checkemojie(text):\n",
        "        emojistext=[]\n",
        "        for char in text:\n",
        "            if any(emoji.distinct_emoji_list(char)) and char in emojis.keys():\n",
        "                emojistext.append(emojis[emoji.distinct_emoji_list(char)[0]])\n",
        "        return \" \".join(emojistext)\n",
        "\n",
        "    def emojiTextTransform(text):\n",
        "        cleantext=re.sub(r'[^\\w\\s]','',text)\n",
        "        return cleantext+\" \"+checkemojie(text)\n",
        "\n",
        "\n",
        "    # Remove emojis\n",
        "    no_emojis = emojiTextTransform(no_punc)\n",
        "\n",
        "    # Remove numbers\n",
        "    text_no_numbers = re.sub(r'[0-9]', ' ', no_emojis)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    text_no_stopwords = nltk.wordpunct_tokenize(text_no_numbers)\n",
        "    words_no_stopwords = [word for word in text_no_stopwords if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words_no_stopwords]\n",
        "\n",
        "    # Join the words into a single string\n",
        "    cleaned_text = ' '.join(lemmatized_words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Example usage:\n",
        "df = pd.read_excel(\"/content/train.xlsx\")\n",
        "df = pd.DataFrame(df,columns=['review_description', 'rating'])\n",
        "df['clean_text'] = df['review_description'].apply(clean_text)\n",
        "print(df['clean_text'] )\n"
      ],
      "metadata": {
        "id": "c3GXHLFE4pbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6962789b-232b-4d99-8526-9f8b99b98246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        شركه زباله سواقين بتبرشم مفيش حتي رقم للشكاوي ...\n",
            "1        خدمة الدفع طريق الكي نت توقفت عندي اصبح فقط ال...\n",
            "2        تطبيق غبي جاري حذفه عاملين اكواد خصم نستخدمها ...\n",
            "3        فعلا تطبيق ممتاز فى امكانية يتيح لمستخدم التطب...\n",
            "4                سيء جدا اسعار رسوم التوصيل تمت للواقع صله\n",
            "                               ...                        \n",
            "32031    التطبيق اصبح سيء للغايه نقوم بطلب يتم وصول الط...\n",
            "32032                                                     \n",
            "32033                 الباقه بتخلص وبشحن مرتين باقه اضافيه\n",
            "32034    تطبيق فاشل وصلني الطلب ناقص ومش ينفع اعمل حاجة...\n",
            "32035                                         ليش يفتح معي\n",
            "Name: clean_text, Length: 32036, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-bidi\n",
        "%pip install langid\n",
        "%pip install keras_preprocessing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGcj6qiyNYcL",
        "outputId": "7f2e7b44-e11b-47ad-d632-cd3663ee335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=dab5bbe60f87ab15849d574d0a12d906840f13421583d39b1f7a178448151a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras.preprocessing.sequence\n",
        "\n",
        "tokenizer = Tokenizer(num_words=7000)  # Adjust based on your actual vocabulary size\n",
        "class NN:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def start(self):\n",
        "      df = self.df\n",
        "\n",
        "      # Apply text cleaning function to 'review_description' and create 'clean_text' column\n",
        "      df['clean_text'] = df['review_description'].apply(clean_text)\n",
        "\n",
        "      mapping = {-1: 0, 0: 1, 1: 2}\n",
        "      df['rating'] = df['rating'].replace(mapping)\n",
        "\n",
        "      # Adjust the vocabulary size in the Tokenizer\n",
        "\n",
        "      tokenizer.fit_on_texts(df['clean_text'])\n",
        "\n",
        "      X = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "      X = keras.preprocessing.sequence.pad_sequences(X, maxlen=16)\n",
        "\n",
        "      model = self.build_lstm_model_with_embedding(input_shape=(X.shape[1],), output_units=3, vocab_size=7000)\n",
        "      model.summary()\n",
        "      model.fit(X, df['rating'], epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "      self.model = model  # Save the model for later use\n",
        "\n",
        "    def build_lstm_model_with_embedding(self, input_shape, output_units=3, vocab_size=5000, embedding_dim=50):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Adding an Embedding layer\n",
        "        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_shape[0]))\n",
        "\n",
        "\n",
        "        # Adding an LSTM layer\n",
        "        model.add(LSTM(units=128, activation='relu', return_sequences=True))  # Set return_sequences=True for subsequent LSTM layers\n",
        "\n",
        "        # Adding a dropout layer\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # Adding another LSTM layer\n",
        "        model.add(LSTM(units=128, activation='relu'))\n",
        "\n",
        "        # Adding another dropout layer\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # Output layer with softmax activation for classification\n",
        "        model.add(Dense(units=output_units, activation='softmax'))\n",
        "\n",
        "        # Compiling the model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Assuming X_test is a DataFrame with a column 'review_description'\n",
        "        X_test['clean_text'] = X_test['review_description'].apply(clean_text)\n",
        "\n",
        "        # Tokenize and pad the sequences\n",
        "        X_test_sequences = tokenizer.texts_to_sequences(X_test['clean_text'])\n",
        "        X_test_padded = keras.preprocessing.sequence.pad_sequences(X_test_sequences, maxlen=16)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X_test_padded)\n",
        "\n",
        "\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "8oIV4bhPNPm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "data = pd.read_excel(\"/content/train.xlsx\")\n",
        "data = pd.DataFrame(data, columns=['review_description', 'rating'])\n",
        "\n",
        "# Create NN object and train the model\n",
        "nn_model = NN(data)\n",
        "nn_model.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljzysF2TTJ3d",
        "outputId": "92a6c1e8-3bf1-4142-bf40-30de45bcc953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 16, 50)            350000    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 16, 128)           91648     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16, 128)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 573619 (2.19 MB)\n",
            "Trainable params: 573619 (2.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "801/801 [==============================] - 33s 39ms/step - loss: 0.5678 - accuracy: 0.7846 - val_loss: 0.4785 - val_accuracy: 0.8237\n",
            "Epoch 2/10\n",
            "801/801 [==============================] - 31s 39ms/step - loss: 0.4301 - accuracy: 0.8496 - val_loss: 0.4886 - val_accuracy: 0.8198\n",
            "Epoch 3/10\n",
            "801/801 [==============================] - 31s 39ms/step - loss: 0.3733 - accuracy: 0.8649 - val_loss: 0.5476 - val_accuracy: 0.8198\n",
            "Epoch 4/10\n",
            "801/801 [==============================] - 31s 39ms/step - loss: 0.3275 - accuracy: 0.8831 - val_loss: 0.5520 - val_accuracy: 0.8121\n",
            "Epoch 5/10\n",
            "801/801 [==============================] - 32s 39ms/step - loss: 0.3040 - accuracy: 0.8921 - val_loss: 0.6892 - val_accuracy: 0.8118\n",
            "Epoch 6/10\n",
            "801/801 [==============================] - 32s 40ms/step - loss: 0.2708 - accuracy: 0.9036 - val_loss: 0.7551 - val_accuracy: 0.8051\n",
            "Epoch 7/10\n",
            "801/801 [==============================] - 32s 39ms/step - loss: 0.2468 - accuracy: 0.9111 - val_loss: 0.8521 - val_accuracy: 0.7954\n",
            "Epoch 8/10\n",
            "801/801 [==============================] - 33s 41ms/step - loss: 0.2368 - accuracy: 0.9152 - val_loss: 0.8655 - val_accuracy: 0.7965\n",
            "Epoch 9/10\n",
            "801/801 [==============================] - 32s 40ms/step - loss: 0.2167 - accuracy: 0.9218 - val_loss: 0.8427 - val_accuracy: 0.7962\n",
            "Epoch 10/10\n",
            "801/801 [==============================] - 32s 40ms/step - loss: 0.2010 - accuracy: 0.9272 - val_loss: 0.9859 - val_accuracy: 0.7945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "X_test = pd.read_csv(\"/content/test _no_label.csv\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = nn_model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions,axis=1)\n",
        "# Define the mapping dictionary\n",
        "mapping = {0: -1, 1: 0, 2: 1}\n",
        "\n",
        "# Use the mapping dictionary to replace the values in the array\n",
        "mapped_data = np.array([mapping[value] for value in predicted_labels])\n",
        "\n",
        "# Print the mapped array\n",
        "print(mapped_data)"
      ],
      "metadata": {
        "id": "ZLzcJ4YKcLUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15abe0d8-c6c2-41f1-c6b4-9331ed3a621b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 1s 10ms/step\n",
            "[ 1  1 -1  1  1  1 -1 -1  1 -1  1  1  1 -1  0  1  1 -1 -1 -1  1  1  1  1\n",
            "  0 -1  1  1  1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1 -1  1 -1\n",
            "  1  1  1  1 -1 -1 -1  1 -1  0 -1 -1  1  1 -1  0  1  1 -1  1 -1 -1 -1  1\n",
            "  1 -1 -1 -1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
            " -1 -1  1  0 -1 -1  1 -1  1  1 -1 -1  1  1  1  1  1  1  1  0  1  1  1  1\n",
            "  1  1  1  1  1  1  1 -1  1  1 -1 -1  1  1  1 -1  1  1  0  1  1 -1  1  1\n",
            "  1 -1 -1 -1  1  1  1  1  1  0 -1  1  1 -1  0  1  1  1  0  1 -1 -1 -1  1\n",
            "  1  1  1 -1  1  1  1  1  1  0  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "  1  1 -1  1 -1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1  0  1  1 -1 -1  1\n",
            "  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1  1  1  1  1  1 -1  1  1 -1\n",
            " -1 -1  1  1 -1  1  1 -1  1  1  1 -1  1 -1 -1  1 -1  1 -1  1  0  1  0  1\n",
            "  1  1 -1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1  1  1  1  1 -1 -1  1  1  1\n",
            "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1 -1 -1  1  0 -1 -1  1 -1  1  1  1  1  1 -1  1  0  1  1  1  1\n",
            "  0  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1 -1 -1  1  1  1  1  0  1 -1\n",
            "  1 -1  0 -1 -1  1  1 -1 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1  1 -1 -1\n",
            "  1  1  1  1  1 -1  1  1  1 -1  1  1  1 -1 -1  1  1  1  1  1 -1  1 -1 -1\n",
            "  1  1  1  1 -1  1  1 -1 -1 -1  1 -1  1  1  1  1  1 -1  1  1 -1 -1 -1  1\n",
            "  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1 -1  1  1 -1  1  1 -1 -1  1  1\n",
            "  1  1 -1  1  1  1  1 -1  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
            "  1 -1  1  1  1  1  1  1  1  1 -1 -1  1 -1  0  1  1  1  1  1 -1 -1  1  1\n",
            "  1  1 -1 -1 -1 -1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1\n",
            " -1  1 -1  1  1 -1 -1 -1  1  1  1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1\n",
            "  1  1  1  1  1  1  1  1 -1 -1  1  1  1 -1 -1 -1  1  1 -1  1 -1  1  1 -1\n",
            "  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1  1  1  1\n",
            "  1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  0  1  1  1  1  1  1  1 -1\n",
            " -1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
            "  1  1  1  0  1  1  1 -1 -1  1 -1  1 -1  1  1  1  1 -1 -1 -1  1 -1  1 -1\n",
            "  1 -1 -1  1  1 -1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1  1  1 -1\n",
            "  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  0  0 -1\n",
            "  1  0  1 -1 -1  1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1  1 -1 -1  1  1\n",
            "  1  1 -1  1  1 -1  1 -1 -1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1 -1 -1\n",
            "  1  1  1  1  1  1 -1 -1 -1  1  0  1  1  1  1  1 -1  1  1  0  1  1  1 -1\n",
            "  1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  0  1  1 -1  1 -1  1\n",
            " -1  1  1  1  0  1 -1  1  1  1  1  1  0 -1  1 -1  1  1 -1  1  1  1  1 -1\n",
            "  1  1  1  1  1 -1  1 -1  0  1  1  1 -1 -1 -1  1  1  0  1 -1 -1  1  1 -1\n",
            "  1  1  1  1  1  1  1  1 -1 -1  1  1 -1  0  1  1  1  1 -1  1  1  1  1  1\n",
            " -1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  0 -1  1  0 -1  1 -1\n",
            "  1  1  1  1 -1  1  0 -1 -1  1  1  1  1  1  1 -1  1  1 -1  0 -1  1  1 -1\n",
            "  0  1  1  1  1  1 -1 -1 -1  1  1  1  0  1  1  1 -1 -1  1 -1 -1 -1 -1  1\n",
            "  1 -1 -1  1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1  1 -1  1  1  1 -1 -1\n",
            "  1 -1  0 -1  1  1  1  1  1  1  1  1 -1  1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission=pd.DataFrame()\n",
        "submission['ID']=X_test['ID']\n",
        "submission['rating']=mapped_data\n",
        "\n",
        "print(submission)\n",
        "submission.to_csv(\"neural_project5.csv\",index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSII6opIWdQz",
        "outputId": "6c7d5d9b-514a-4091-d0e2-cd7f37838a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ID  rating\n",
            "0       1       1\n",
            "1       2       1\n",
            "2       3       1\n",
            "3       4       1\n",
            "4       5       1\n",
            "..    ...     ...\n",
            "995   996       1\n",
            "996   997      -1\n",
            "997   998       1\n",
            "998   999      -1\n",
            "999  1000      -1\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Model**"
      ],
      "metadata": {
        "id": "53ew-r5My8w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "\n",
        "# Custom layer for multi-head self-attention\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        # Dimensionality of the embedding and number of attention heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each attention head\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Ensure embedding dimension is divisible by the number of heads\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_dim\n",
        "        ), \"Embedding dimension needs to be divisible by the number of heads\"\n",
        "\n",
        "        # Linear transformations for query, key, value, and output\n",
        "        self.wq = layers.Dense(embed_dim)\n",
        "        self.wk = layers.Dense(embed_dim)\n",
        "        self.wv = layers.Dense(embed_dim)\n",
        "        self.dense = layers.Dense(embed_dim)\n",
        "\n",
        "    # Helper function to split heads of the input tensor\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # Forward pass of the multi-head self-attention layer\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear transformations for query, key, and value\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # Splitting into multiple heads\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Scaled dot-product attention mechanism\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "\n",
        "        # Rearrange and concatenate attention heads\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "        # Linear transformation for the output\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    # Scaled dot-product attention mechanism\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Apply mask to prevent attending to padded positions\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += mask * -1e9\n",
        "\n",
        "        # Softmax activation to get attention weights\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        # Weighted sum using attention weights\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Custom layer for a Transformer block\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # Multi-head self-attention layer\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        # Feedforward neural network\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        # Layer normalization for both attention and feedforward output\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Dropout layers for regularization\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    # Forward pass of the Transformer block\n",
        "    def call(self, inputs, training=True):\n",
        "        # Multi-head self-attention layer\n",
        "        attn_output, _ = self.att(inputs, inputs, inputs, mask=None)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Feedforward neural network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "# Custom layer for token and position embedding\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        # Embedding layer for tokens\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        # Embedding layer for positions\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Forward pass of the token and position embedding layer\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        # Create positions tensor\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        # Token embedding and addition with position embeddings\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "# Function to create the entire transformer model\n",
        "def create_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes):\n",
        "    inputs = layers.Input(shape=(maxlen,))\n",
        "    # Token and position embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Stack multiple Transformer blocks\n",
        "    for _ in range(num_blocks):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Global average pooling to reduce spatial dimensions\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    # Dropout layer for regularization\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    # Dense layer with ReLU activation for additional non-linearity\n",
        "    x = layers.Dense(20, activation=\"relu\")(x)\n",
        "    # Additional dropout layer\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    # Output layer with softmax activation for classification\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "nj65c758ytaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Training**"
      ],
      "metadata": {
        "id": "qiLSSG_Sy4Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from an Excel file into a pandas DataFrame\n",
        "df = pd.read_excel(\"train.xlsx\")\n",
        "\n",
        "# Apply a text cleaning function to the 'review_description' column\n",
        "df['review_description'] = df['review_description'].apply(lambda x: clean_text(x))\n",
        "\n",
        "# Tokenize and pad text data\n",
        "max_len = 100\n",
        "\n",
        "# Initialize a tokenizer and fit it on the preprocessed text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['review_description'])\n",
        "\n",
        "# Convert text sequences to numerical sequences and pad to a fixed length\n",
        "sequences = tokenizer.texts_to_sequences(df['review_description'])\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Extract labels and add 1 to each label to make them 0, 1, 2\n",
        "y = df['rating'].values + 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Calculate the vocabulary size for the tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Model configuration parameters\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "num_blocks = 3\n",
        "num_classes = 3\n",
        "\n",
        "# Build and compile the transformer model\n",
        "model = create_transformer_model(max_len, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Training configuration parameters\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "# Train the model on the training set, with early stopping to prevent overfitting\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"transformer_model.keras\")\n"
      ],
      "metadata": {
        "id": "zuHeOrbOy2a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Testing**"
      ],
      "metadata": {
        "id": "_nyg6mCTy_jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the testing dataset\n",
        "test_df = pd.read_csv('test _no_label.csv')\n",
        "\n",
        "# Tokenize and pad the text data in the new DataFrame\n",
        "new_sequences = tokenizer.texts_to_sequences(test_df['review_description'])\n",
        "new_X = pad_sequences(new_sequences, maxlen=max_len)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = model.predict(new_X)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "predicted_labels = predictions.argmax(axis=1) - 1  # Subtract 1 to convert back to -1, 0, 1\n",
        "\n",
        "# Add the predicted labels to the new DataFrame\n",
        "test_df['rating'] = predicted_labels\n",
        "test_df = test_df[['ID', 'rating']]\n",
        "\n",
        "test_df.to_csv('Results.csv', index = False)"
      ],
      "metadata": {
        "id": "ZR-q3TYYzChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfqLAd_IwhXW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}