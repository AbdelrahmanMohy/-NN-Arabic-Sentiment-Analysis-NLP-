{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n",
        "from deep_translator import GoogleTranslator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_18vdbq-ErL3",
        "outputId": "a9ea0108-20e8-4284-b41a-21408d648468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.31.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2023.11.17)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d6Gi5_bjCTTl",
        "outputId": "2ef00912-1ed0-4ebe-bfb2-fd613de5427c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.9.0-py2.py3-none-any.whl (397 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/397.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m266.2/397.5 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m397.5/397.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk as nltk\n",
        "import re\n",
        "from string import punctuation\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import emoji\n",
        "\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "\n",
        "from nltk import pos_tag\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQUvXWV0zRIu",
        "outputId": "69ff924d-59e7-4d3f-abf1-a0a7afaf5280"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel(\"/content/train.xlsx\")\n",
        "data = pd.DataFrame(data,columns=['review_description', 'rating'])\n",
        "data.columns = ['review_description','rating']\n",
        "\n",
        "# Extract feature & label\n",
        "X = data['review_description']\n",
        "Y = data['rating']\n",
        "print(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NkFsfgA3s-E",
        "outputId": "fb27a953-1282-4564-cfe6-ac8376975a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        ÿ¥ÿ±ŸÉŸá ÿ≤ÿ®ÿßŸÑŸá Ÿà ÿ≥ŸàÿßŸÇŸäŸÜ ÿ®ÿ™ÿ®ÿ±ÿ¥ŸÖ Ÿà ŸÖŸÅŸäÿ¥ ÿ≠ÿ™Ÿä ÿ±ŸÇŸÖ ŸÑŸÑÿ¥ŸÉ...\n",
            "1        ÿÆÿØŸÖÿ© ÿßŸÑÿØŸÅÿπ ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿßŸÑŸÉŸä ŸÜÿ™ ÿ™ŸàŸÇŸÅÿ™ ÿπŸÜÿØŸä ÿßÿµÿ®ÿ≠ ŸÅŸÇÿ∑...\n",
            "2        ÿ™ÿ∑ÿ®ŸäŸÇ ÿ∫ÿ®Ÿä Ÿà ÿ¨ÿßÿ±Ÿä ÿ≠ÿ∞ŸÅŸá ÿå ÿπÿßŸÖŸÑŸäŸÜ ÿßŸÉŸàÿßÿØ ÿÆÿµŸÖ Ÿà ŸÑŸÖÿß...\n",
            "3        ŸÅÿπŸÑÿß ÿ™ÿ∑ÿ®ŸäŸÇ ŸÖŸÖÿ™ÿßÿ≤ ÿ®ÿ≥ ŸÑŸà ŸÅŸâ ÿßŸÖŸÉÿßŸÜŸäÿ© Ÿäÿ™Ÿäÿ≠ ŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ...\n",
            "4        ÿ≥Ÿäÿ° ÿ¨ÿØÿß ÿå ÿßÿ≥ÿπÿßÿ± ÿ±ÿ≥ŸàŸÖ ÿßŸÑÿ™ŸàÿµŸäŸÑ ŸÑÿß ÿ™ŸÖÿ™ ŸÑŸÑŸàÿßŸÇÿπ ÿ® ÿµ...\n",
            "                               ...                        \n",
            "32031    ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ÿßÿµÿ®ÿ≠ ÿ≥Ÿäÿ° ŸÑŸÑÿ∫ÿßŸäŸá ŸÜŸÇŸàŸÖ ÿ®ÿ∑ŸÑÿ® ŸÑÿß Ÿäÿ™ŸÖ ŸàÿµŸàŸÑ ...\n",
            "32032                                           y love you\n",
            "32033        ÿßŸÑÿ®ÿßŸÇŸá ÿ®ÿ™ÿÆŸÑÿµ Ÿàÿ®ÿ¥ÿ≠ŸÜ ŸÖÿ±ÿ™ŸäŸÜ ÿ®ÿßŸÇŸá ÿßÿ∂ÿßŸÅŸäŸá Ÿ°Ÿ†Ÿ† ÿ¨ŸÜŸäŸá\n",
            "32034    ÿ™ÿ∑ÿ®ŸäŸÇ ŸÅÿßÿ¥ŸÑ ŸàÿµŸÑŸÜŸä ÿßŸÑÿ∑ŸÑÿ® ŸÜÿßŸÇÿµ ŸàŸÖÿ¥ ŸäŸÜŸÅÿπ ÿßÿπŸÖŸÑ ÿ≠ÿßÿ¨ÿ©...\n",
            "32035                                      ŸÑŸäÿ¥ ŸÖÿß ŸäŸÅÿ™ÿ≠ ŸÖÿπŸä\n",
            "Name: review_description, Length: 32036, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "import emoji\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def clean_text(text):\n",
        "    # Translate text to Arabic\n",
        "\n",
        "\n",
        "    punctuation_re = '[?ÿü!Ÿ™,ÿå@#$%&*‚Ç¨+-¬£_~\\‚ÄúÃØ/=><.\\€∞):ÿõ}{√∑%(\"\\'ŸéŸãŸèŸåŸêŸçŸëŸíŸ†-Ÿ©]'\n",
        "\n",
        "    emoji_re = re.compile(\"[\"\n",
        "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "            u\"\\U00002702-\\U000027B0\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U000024C2-\\U0001F251\"\n",
        "            u\"\\U0001f926-\\U0001f937\"\n",
        "            u\"\\U00010000-\\U0010ffff\"\n",
        "            u\"\\u2640-\\u2642\"\n",
        "            u\"\\u2600-\\u2B55\"\n",
        "            u\"\\u200d\"\n",
        "            u\"\\u23cf\"\n",
        "            u\"\\u23e9\"\n",
        "            u\"\\u231a\"\n",
        "            u\"\\ufe0f\"  # dingbats\n",
        "            u\"\\u3030\"\n",
        "            \"]+\", flags=re.UNICODE)\n",
        "\n",
        "    stop_words = set(stopwords.words(\"arabic\"))\n",
        "    stemmer = SnowballStemmer('arabic')\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    # Remove punctuation\n",
        "    no_punc = re.sub(punctuation_re, ' ', str(text))\n",
        "    emojis = {\n",
        "        \"üôÇ\":\"Ÿäÿ®ÿ™ÿ≥ŸÖ\",\n",
        "        \"üòÇ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üíî\":\"ŸÇŸÑÿ® ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"üôÇ\":\"Ÿäÿ®ÿ™ÿ≥ŸÖ\",\n",
        "        \"‚ù§\":\"ÿ≠ÿ®\",\n",
        "        \"‚ù§\":\"ÿ≠ÿ®\",\n",
        "        \"üòç\":\"ÿ≠ÿ®\",\n",
        "        \"üò≠\":\"Ÿäÿ®ŸÉŸä\",\n",
        "        \"üò¢\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        \"üòî\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        \"‚ô•\":\"ÿ≠ÿ®\",\n",
        "        \"üíú\":\"ÿ≠ÿ®\",\n",
        "        \"üòÖ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üôÅ\":\"ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"üíï\":\"ÿ≠ÿ®\",\n",
        "        \"üíô\":\"ÿ≠ÿ®\",\n",
        "        \"üòû\":\"ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"üòä\":\"ÿ≥ÿπÿßÿØÿ©\",\n",
        "        \"üëè\":\"ŸäÿµŸÅŸÇ\",\n",
        "        \"üëå\":\"ÿßÿ≠ÿ≥ŸÜÿ™\",\n",
        "        \"üò¥\":\"ŸäŸÜÿßŸÖ\",\n",
        "        \"üòÄ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üòå\":\"ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"üåπ\":\"Ÿàÿ±ÿØÿ©\",\n",
        "        \"üôà\":\"ÿ≠ÿ®\",\n",
        "        \"üòÑ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üòê\":\"ŸÖÿ≠ÿßŸäÿØ\",\n",
        "        \"‚úå\":\"ŸÖŸÜÿ™ÿµÿ±\",\n",
        "        \"‚ú®\":\"ŸÜÿ¨ŸÖŸá\",\n",
        "        \"ü§î\":\"ÿ™ŸÅŸÉŸäÿ±\",\n",
        "        \"üòè\":\"Ÿäÿ≥ÿ™Ÿáÿ≤ÿ°\",\n",
        "        \"üòí\":\"Ÿäÿ≥ÿ™Ÿáÿ≤ÿ°\",\n",
        "        \"üôÑ\":\"ŸÖŸÑŸÑ\",\n",
        "        \"üòï\":\"ÿπÿµÿ®Ÿäÿ©\",\n",
        "        \"üòÉ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üå∏\":\"Ÿàÿ±ÿØÿ©\",\n",
        "        \"üòì\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        \"üíû\":\"ÿ≠ÿ®\",\n",
        "        \"üíó\":\"ÿ≠ÿ®\",\n",
        "        \"üòë\":\"ŸÖŸÜÿ≤ÿπÿ¨\",\n",
        "        \"üí≠\":\"ÿ™ŸÅŸÉŸäÿ±\",\n",
        "        \"üòé\":\"ÿ´ŸÇÿ©\",\n",
        "        \"üíõ\":\"ÿ≠ÿ®\",\n",
        "        \"üò©\":\"ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"üí™\":\"ÿπÿ∂ŸÑÿßÿ™\",\n",
        "        \"üëç\":\"ŸÖŸàÿßŸÅŸÇ\",\n",
        "        \"üôèüèª\":\"ÿ±ÿ¨ÿßÿ° ÿ∑ŸÑÿ®\",\n",
        "        \"üò≥\":\"ŸÖÿµÿØŸàŸÖ\",\n",
        "        \"üëèüèº\":\"ÿ™ÿµŸÅŸäŸÇ\",\n",
        "        \"üé∂\":\"ŸÖŸàÿ≥ŸäŸÇŸä\",\n",
        "        \"üåö\":\"ÿµŸÖÿ™\",\n",
        "        \"üíö\":\"ÿ≠ÿ®\",\n",
        "        \"üôè\":\"ÿ±ÿ¨ÿßÿ° ÿ∑ŸÑÿ®\",\n",
        "        \"üíò\":\"ÿ≠ÿ®\",\n",
        "        \"üçÉ\":\"ÿ≥ŸÑÿßŸÖ\",\n",
        "        \"‚ò∫\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üê∏\":\"ÿ∂ŸÅÿØÿπ\",\n",
        "        \"üò∂\":\"ŸÖÿµÿØŸàŸÖ\",\n",
        "        \"‚úå\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"‚úãüèª\":\"ÿ™ŸàŸÇŸÅ\",\n",
        "        \"üòâ\":\"ÿ∫ŸÖÿ≤ÿ©\",\n",
        "        \"üå∑\":\"ÿ≠ÿ®\",\n",
        "        \"üôÉ\":\"ŸÖÿ®ÿ™ÿ≥ŸÖ\",\n",
        "        \"üò´\":\"ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"üò®\":\"ŸÖÿµÿØŸàŸÖ\",\n",
        "        \"üéº \":\"ŸÖŸàÿ≥ŸäŸÇŸä\",\n",
        "        \"üçÅ\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üçÇ\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üíü\":\"ÿ≠ÿ®\",\n",
        "        \"üò™\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        \"üòÜ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üò£\":\"ÿßÿ≥ÿ™Ÿäÿßÿ°\",\n",
        "        \"‚ò∫\":\"ÿ≠ÿ®\",\n",
        "        \"üò±\":\"ŸÉÿßÿ±ÿ´ÿ©\",\n",
        "        \"üòÅ\":\"Ÿäÿ∂ÿ≠ŸÉ\",\n",
        "        \"üòñ\":\"ÿßÿ≥ÿ™Ÿäÿßÿ°\",\n",
        "        \"üèÉüèº\":\"Ÿäÿ¨ÿ±Ÿä\",\n",
        "        \"üò°\":\"ÿ∫ÿ∂ÿ®\",\n",
        "        \"üö∂\":\"Ÿäÿ≥Ÿäÿ±\",\n",
        "        \"ü§ï\":\"ŸÖÿ±ÿ∂\",\n",
        "        \"‚Äº\":\"ÿ™ÿπÿ¨ÿ®\",\n",
        "        \"üïä\":\"ÿ∑ÿßÿ¶ÿ±\",\n",
        "        \"üëåüèª\":\"ÿßÿ≠ÿ≥ŸÜÿ™\",\n",
        "        \"‚ù£\":\"ÿ≠ÿ®\",\n",
        "        \"üôä\":\"ŸÖÿµÿØŸàŸÖ\",\n",
        "        \"üíÉ\":\"ÿ≥ÿπÿßÿØÿ© ŸÖÿ±ÿ≠\",\n",
        "        \"üíÉüèº\":\"ÿ≥ÿπÿßÿØÿ© ŸÖÿ±ÿ≠\",\n",
        "        \"üòú\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üëä\":\"ÿ∂ÿ±ÿ®ÿ©\",\n",
        "        \"üòü\":\"ÿßÿ≥ÿ™Ÿäÿßÿ°\",\n",
        "        \"üíñ\":\"ÿ≠ÿ®\",\n",
        "        \"üò•\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        \"üéª\":\"ŸÖŸàÿ≥ŸäŸÇŸä\",\n",
        "        \"‚úí\":\"ŸäŸÉÿ™ÿ®\",\n",
        "        \"üö∂üèª\":\"Ÿäÿ≥Ÿäÿ±\",\n",
        "        \"üíé\":\"ÿßŸÑŸÖÿßÿ∏\",\n",
        "        \"üò∑\":\"Ÿàÿ®ÿßÿ° ŸÖÿ±ÿ∂\",\n",
        "        \"‚òù\":\"Ÿàÿßÿ≠ÿØ\",\n",
        "        \"üö¨\":\"ÿ™ÿØÿÆŸäŸÜ\",\n",
        "        \"üíê\" : \"Ÿàÿ±ÿØ\",\n",
        "        \"üåû\" : \"ÿ¥ŸÖÿ≥\",\n",
        "        \"üëÜ\" : \"ÿßŸÑÿßŸàŸÑ\",\n",
        "        \"‚ö†\" :\"ÿ™ÿ≠ÿ∞Ÿäÿ±\",\n",
        "        \"ü§ó\" : \"ÿßÿ≠ÿ™Ÿàÿßÿ°\",\n",
        "        \"‚úñ\": \"ÿ∫ŸÑÿ∑\",\n",
        "        \"üìç\"  : \"ŸÖŸÉÿßŸÜ\",\n",
        "        \"üë∏\" : \"ŸÖŸÑŸÉŸá\",\n",
        "        \"üëë\" : \"ÿ™ÿßÿ¨\",\n",
        "        \"‚úî\" : \"ÿµÿ≠\",\n",
        "        \"üíå\": \"ŸÇŸÑÿ®\",\n",
        "        \"üò≤\" : \"ŸÖŸÜÿØŸáÿ¥\",\n",
        "        \"üí¶\": \"ŸÖÿßÿ°\",\n",
        "        \"üö´\" : \"ÿÆÿ∑ÿß\",\n",
        "        \"üëèüèª\" : \"ÿ®ÿ±ÿßŸÅŸà\",\n",
        "        \"üèä\" :\"Ÿäÿ≥ÿ®ÿ≠\",\n",
        "        \"üëçüèª\": \"ÿ™ŸÖÿßŸÖ\",\n",
        "        \"‚≠ï\" :\"ÿØÿßÿ¶ÿ±Ÿá ŸÉÿ®Ÿäÿ±Ÿá\",\n",
        "        \"üé∑\" : \"ÿ≥ÿßŸÉÿ≥ŸÅŸàŸÜ\",\n",
        "        \"üëã\": \"ÿ™ŸÑŸàŸäÿ≠ ÿ®ÿßŸÑŸäÿØ\",\n",
        "        \"‚úåüèº\": \"ÿπŸÑÿßŸÖŸá ÿßŸÑŸÜÿµÿ±\",\n",
        "        \"üåù\":\"ŸÖÿ®ÿ™ÿ≥ŸÖ\",\n",
        "        \"‚ûø\"  : \"ÿπŸÇÿØŸá ŸÖÿ≤ÿØŸàÿ¨Ÿá\",\n",
        "        \"üí™üèº\" : \"ŸÇŸàŸä\",\n",
        "        \"üì©\":  \"ÿ™ŸàÿßÿµŸÑ ŸÖÿπŸä\",\n",
        "        \"‚òï\": \"ŸÇŸáŸàŸá\",\n",
        "        \"üòß\" : \"ŸÇŸÑŸÇ Ÿà ÿµÿØŸÖÿ©\",\n",
        "        \"üó®\": \"ÿ±ÿ≥ÿßŸÑÿ©\",\n",
        "        \"‚ùó\" :\"ÿ™ÿπÿ¨ÿ®\",\n",
        "        \"üôÜüèª\": \"ÿßÿ¥ÿßÿ±Ÿá ŸÖŸàÿßŸÅŸÇŸá\",\n",
        "        \"üëØ\" :\"ÿßÿÆŸàÿßÿ™\",\n",
        "        \"¬©\" :  \"ÿ±ŸÖÿ≤\",\n",
        "        \"üëµüèΩ\" :\"ÿ≥ŸäÿØŸá ÿπÿ¨Ÿàÿ≤Ÿá\",\n",
        "        \"üê£\": \"ŸÉÿ™ŸÉŸàÿ™\",\n",
        "        \"üôå\": \"ÿ™ÿ¥ÿ¨Ÿäÿπ\",\n",
        "        \"üôá\": \"ÿ¥ÿÆÿµ ŸäŸÜÿ≠ŸÜŸä\",\n",
        "        \"üëêüèΩ\":\"ÿßŸäÿØŸä ŸÖŸÅÿ™Ÿàÿ≠Ÿá\",\n",
        "        \"üëåüèΩ\": \"ÿ®ÿßŸÑÿ∏ÿ®ÿ∑\",\n",
        "        \"‚Åâ\" : \"ÿßÿ≥ÿ™ŸÜŸÉÿßÿ±\",\n",
        "        \"‚öΩ\": \"ŸÉŸàÿ±Ÿá\",\n",
        "        \"üï∂\" :\"ÿ≠ÿ®\",\n",
        "        \"üéà\" :\"ÿ®ÿßŸÑŸàŸÜ\",\n",
        "        \"üéÄ\":    \"Ÿàÿ±ÿØŸá\",\n",
        "        \"üíµ\":  \"ŸÅŸÑŸàÿ≥\",\n",
        "        \"üòã\":  \"ÿ¨ÿßÿ¶ÿπ\",\n",
        "        \"üòõ\":  \"Ÿäÿ∫Ÿäÿ∏\",\n",
        "        \"üò†\":  \"ÿ∫ÿßÿ∂ÿ®\",\n",
        "        \"‚úçüèª\":  \"ŸäŸÉÿ™ÿ®\",\n",
        "        \"üåæ\":  \"ÿßÿ±ÿ≤\",\n",
        "        \"üë£\":  \"ÿßÿ´ÿ± ŸÇÿØŸÖŸäŸÜ\",\n",
        "        \"‚ùå\":\"ÿ±ŸÅÿ∂\",\n",
        "        \"üçü\":\"ÿ∑ÿπÿßŸÖ\",\n",
        "        \"üë¨\":\"ÿµÿØÿßŸÇÿ©\",\n",
        "        \"üê∞\":\"ÿßÿ±ŸÜÿ®\",\n",
        "        \"‚òÇ\":\"ŸÖÿ∑ÿ±\",\n",
        "        \"‚öú\":\"ŸÖŸÖŸÑŸÉÿ© ŸÅÿ±ŸÜÿ≥ÿß\",\n",
        "        \"üêë\":\"ÿÆÿ±ŸàŸÅ\",\n",
        "        \"üó£\":\"ÿµŸàÿ™ ŸÖÿ±ÿ™ŸÅÿπ\",\n",
        "        \"üëåüèº\":\"ÿßÿ≠ÿ≥ŸÜÿ™\",\n",
        "        \"‚òò\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üòÆ\":\"ÿµÿØŸÖÿ©\",\n",
        "        \"üò¶\":\"ŸÇŸÑŸÇ\",\n",
        "        \"‚≠ï\":\"ÿßŸÑÿ≠ŸÇ\",\n",
        "        \"‚úè\":\"ŸÇŸÑŸÖ\",\n",
        "        \"‚Ñπ\":\"ŸÖÿπŸÑŸàŸÖÿßÿ™\",\n",
        "        \"üôçüèª\":\"ÿ±ŸÅÿ∂\",\n",
        "        \"‚ö™\":\"ŸÜÿ∂ÿßÿ±ÿ© ŸÜŸÇÿßÿ°\",\n",
        "        \"üê§\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        \"üí´\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üíù\":\"ÿ≠ÿ®\",\n",
        "        \"üçî\":\"ÿ∑ÿπÿßŸÖ\",\n",
        "        \"‚ù§\":\"ÿ≠ÿ®\",\n",
        "        \"‚úà\":\"ÿ≥ŸÅÿ±\",\n",
        "        \"üèÉüèª‚Äç‚ôÄ\":\"Ÿäÿ≥Ÿäÿ±\",\n",
        "        \"üç≥\":\"ÿ∞ŸÉÿ±\",\n",
        "        \"üé§\":\"ŸÖÿßŸäŸÉ ÿ∫ŸÜÿßÿ°\",\n",
        "        \"üéæ\":\"ŸÉÿ±Ÿá\",\n",
        "        \"üêî\":\"ÿØÿ¨ÿßÿ¨ÿ©\",\n",
        "        \"üôã\":\"ÿ≥ÿ§ÿßŸÑ\",\n",
        "        \"üìÆ\":\"ÿ®ÿ≠ÿ±\",\n",
        "        \"üíâ\":\"ÿØŸàÿßÿ°\",\n",
        "        \"üôèüèº\":\"ÿ±ÿ¨ÿßÿ° ÿ∑ŸÑÿ®\",\n",
        "        \"üíÇüèø \":\"ÿ≠ÿßÿ±ÿ≥\",\n",
        "        \"üé¨\":\"ÿ≥ŸäŸÜŸÖÿß\",\n",
        "        \"‚ô¶\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üí°\":\"ŸÇŸÉÿ±ÿ©\",\n",
        "        \"‚Äº\":\"ÿ™ÿπÿ¨ÿ®\",\n",
        "        \"üëº\":\"ÿ∑ŸÅŸÑ\",\n",
        "        \"üîë\":\"ŸÖŸÅÿ™ÿßÿ≠\",\n",
        "        \"‚ô•\":\"ÿ≠ÿ®\",\n",
        "        \"üïã\":\"ŸÉÿπÿ®ÿ©\",\n",
        "        \"üêì\":\"ÿØÿ¨ÿßÿ¨ÿ©\",\n",
        "        \"üí©\":\"ŸÖÿπÿ™ÿ±ÿ∂\",\n",
        "        \"üëΩ\":\"ŸÅÿ∂ÿßÿ¶Ÿä\",\n",
        "        \"‚òî\":\"ŸÖÿ∑ÿ±\",\n",
        "        \"üç∑\":\"ÿπÿµŸäÿ±\",\n",
        "        \"üåü\":\"ŸÜÿ¨ŸÖÿ©\",\n",
        "        \"‚òÅ\":\"ÿ≥ÿ≠ÿ®\",\n",
        "        \"üëÉ\":\"ŸÖÿπÿ™ÿ±ÿ∂\",\n",
        "        \"üå∫\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üî™\":\"ÿ≥ŸÉŸäŸÜÿ©\",\n",
        "        \"‚ô®\":\"ÿ≥ÿÆŸàŸÜŸäÿ©\",\n",
        "        \"üëäüèº\":\"ÿ∂ÿ±ÿ®\",\n",
        "        \"‚úè\":\"ŸÇŸÑŸÖ\",\n",
        "        \"üö∂üèæ‚Äç‚ôÄ\":\"Ÿäÿ≥Ÿäÿ±\",\n",
        "        \"üëä\":\"ÿ∂ÿ±ÿ®ÿ©\",\n",
        "        \"‚óæ\":\"ŸàŸÇŸÅ\",\n",
        "        \"üòö\":\"ÿ≠ÿ®\",\n",
        "        \"üî∏\":\"ŸÖÿ±ÿ≠\",\n",
        "        \"üëéüèª\":\"ŸÑÿß Ÿäÿπÿ¨ÿ®ŸÜŸä\",\n",
        "        \"üëäüèΩ\":\"ÿ∂ÿ±ÿ®ÿ©\",\n",
        "        \"üòô\":\"ÿ≠ÿ®\",\n",
        "        \"üé•\":\"ÿ™ÿµŸàŸäÿ±\",\n",
        "        \"üëâ\":\"ÿ¨ÿ∞ÿ® ÿßŸÜÿ™ÿ®ÿßŸá\",\n",
        "        \"üëèüèΩ\":\"ŸäÿµŸÅŸÇ\",\n",
        "        \"üí™üèª\":\"ÿπÿ∂ŸÑÿßÿ™\",\n",
        "        \"üè¥\":\"ÿßÿ≥ŸàÿØ\",\n",
        "        \"üî•\":\"ÿ≠ÿ±ŸäŸÇ\",\n",
        "        \"üò¨\":\"ÿπÿØŸÖ ÿßŸÑÿ±ÿßÿ≠ÿ©\",\n",
        "        \"üëäüèø\":\"Ÿäÿ∂ÿ±ÿ®\",\n",
        "        \"üåø\":\"Ÿàÿ±ŸÇŸá ÿ¥ÿ¨ÿ±Ÿá\",\n",
        "        \"‚úãüèº\":\"ŸÉŸÅ ÿßŸäÿØ\",\n",
        "        \"üëê\":\"ÿßŸäÿØŸä ŸÖŸÅÿ™Ÿàÿ≠Ÿá\",\n",
        "        \"‚ò†\":\"Ÿàÿ¨Ÿá ŸÖÿ±ÿπÿ®\",\n",
        "        \"üéâ\":\"ŸäŸáŸÜÿ¶\",\n",
        "        \"üîï\" :\"ÿµÿßŸÖÿ™\",\n",
        "        \"üòø\":\"Ÿàÿ¨Ÿá ÿ≠ÿ≤ŸäŸÜ\",\n",
        "        \"‚òπ\":\"Ÿàÿ¨Ÿá Ÿäÿßÿ¶ÿ≥\",\n",
        "        \"üòò\" :\"ÿ≠ÿ®\",\n",
        "        \"üò∞\" :\"ÿÆŸàŸÅ Ÿà ÿ≠ÿ≤ŸÜ\",\n",
        "        \"üåº\":\"Ÿàÿ±ÿØŸá\",\n",
        "        \"üíã\":  \"ÿ®Ÿàÿ≥Ÿá\",\n",
        "        \"üëá\":\"ŸÑÿßÿ≥ŸÅŸÑ\",\n",
        "        \"‚ù£\":\"ÿ≠ÿ®\",\n",
        "        \"üéß\":\"ÿ≥ŸÖÿßÿπÿßÿ™\",\n",
        "        \"üìù\":\"ŸäŸÉÿ™ÿ®\",\n",
        "        \"üòá\":\"ÿØÿßŸäÿÆ\",\n",
        "        \"üòà\":\"ÿ±ÿπÿ®\",\n",
        "        \"üèÉ\":\"Ÿäÿ¨ÿ±Ÿä\",\n",
        "        \"‚úåüèª\":\"ÿπŸÑÿßŸÖŸá ÿßŸÑŸÜÿµÿ±\",\n",
        "        \"üî´\":\"Ÿäÿ∂ÿ±ÿ®\",\n",
        "        \"‚ùó\":\"ÿ™ÿπÿ¨ÿ®\",\n",
        "        \"üëé\":\"ÿ∫Ÿäÿ± ŸÖŸàÿßŸÅŸÇ\",\n",
        "        \"üîê\":\"ŸÇŸÅŸÑ\",\n",
        "        \"üëà\":\"ŸÑŸÑŸäŸÖŸäŸÜ\",\n",
        "        \"‚Ñ¢\":\"ÿ±ŸÖÿ≤\",\n",
        "        \"üö∂üèΩ\":\"Ÿäÿ™ŸÖÿ¥Ÿä\",\n",
        "        \"üòØ\":\"ŸÖÿ™ŸÅÿßÿ¨ÿ£\",\n",
        "        \"‚úä\":\"ŸäÿØ ŸÖÿ∫ŸÑŸÇŸá\",\n",
        "        \"üòª\":\"ÿßÿπÿ¨ÿßÿ®\",\n",
        "        \"üôâ\" :\"ŸÇÿ±ÿØ\",\n",
        "        \"üëß\":\"ÿ∑ŸÅŸÑŸá ÿµÿ∫Ÿäÿ±Ÿá\",\n",
        "        \"üî¥\":\"ÿØÿßÿ¶ÿ±Ÿá ÿ≠ŸÖÿ±ÿßÿ°\",\n",
        "        \"üí™üèΩ\":\"ŸÇŸàŸá\",\n",
        "        \"üí§\":\"ŸäŸÜÿßŸÖ\",\n",
        "        \"üëÄ\":\"ŸäŸÜÿ∏ÿ±\",\n",
        "        \"‚úçüèª\":\"ŸäŸÉÿ™ÿ®\",\n",
        "        \"‚ùÑ\":\"ÿ™ŸÑÿ¨\",\n",
        "        \"üíÄ\":\"ÿ±ÿπÿ®\",\n",
        "        \"üò§\":\"Ÿàÿ¨Ÿá ÿπÿßÿ®ÿ≥\",\n",
        "        \"üñã\":\"ŸÇŸÑŸÖ\",\n",
        "        \"üé©\":\"ŸÉÿßÿ®\",\n",
        "        \"‚òï\":\"ŸÇŸáŸàŸá\",\n",
        "        \"üòπ\":\"ÿ∂ÿ≠ŸÉ\",\n",
        "        \"üíì\":\"ÿ≠ÿ®\",\n",
        "        \"‚òÑ \":\"ŸÜÿßÿ±\",\n",
        "        \"üëª\":\"ÿ±ÿπÿ®\",\n",
        "        \"‚ùé\":\"ÿÆÿ∑ÿ°\",\n",
        "        \"ü§Æ\":\"ÿ≠ÿ≤ŸÜ\",\n",
        "        'üèª':\"ÿßÿ≠ŸÖÿ±\"\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "    emoticons_to_emoji = {\n",
        "        \":)\" : \"üôÇ\",\n",
        "        \":(\" : \"üôÅ\",\n",
        "        \"xD\" : \"üòÜ\",\n",
        "        \":=(\": \"üò≠\",\n",
        "        \":'(\": \"üò¢\",\n",
        "        \":'‚Äë(\": \"üò¢\",\n",
        "        \"XD\" : \"üòÇ\",\n",
        "        \":D\" : \"üôÇ\",\n",
        "        \"‚ô¨\" : \"ŸÖŸàÿ≥ŸäŸÇŸä\",\n",
        "        \"‚ô°\" : \"‚ù§\",\n",
        "        \"‚òª\"  : \"üôÇ\",\n",
        "        }\n",
        "    def checkemojie(text):\n",
        "        emojistext=[]\n",
        "        for char in text:\n",
        "            if any(emoji.distinct_emoji_list(char)) and char in emojis.keys():\n",
        "                emojistext.append(emojis[emoji.distinct_emoji_list(char)[0]])\n",
        "        return \" \".join(emojistext)\n",
        "\n",
        "    def emojiTextTransform(text):\n",
        "        cleantext=re.sub(r'[^\\w\\s]','',text)\n",
        "        return cleantext+\" \"+checkemojie(text)\n",
        "\n",
        "\n",
        "    # Remove emojis\n",
        "    no_emojis = emojiTextTransform(no_punc)\n",
        "\n",
        "    # Remove numbers\n",
        "    text_no_numbers = re.sub(r'[0-9]', ' ', no_emojis)\n",
        "\n",
        "    # Tokenize and remove stopwords\n",
        "    text_no_stopwords = nltk.wordpunct_tokenize(text_no_numbers)\n",
        "    words_no_stopwords = [word for word in text_no_stopwords if word not in stop_words]\n",
        "\n",
        "    # Lemmatization\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words_no_stopwords]\n",
        "\n",
        "    # Join the words into a single string\n",
        "    cleaned_text = ' '.join(lemmatized_words)\n",
        "\n",
        "    return cleaned_text\n",
        "\n",
        "# Example usage:\n",
        "df = pd.read_excel(\"/content/train.xlsx\")\n",
        "df = pd.DataFrame(df,columns=['review_description', 'rating'])\n",
        "df['clean_text'] = df['review_description'].apply(clean_text)\n",
        "print(df['clean_text'] )\n"
      ],
      "metadata": {
        "id": "c3GXHLFE4pbA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6962789b-232b-4d99-8526-9f8b99b98246"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        ÿ¥ÿ±ŸÉŸá ÿ≤ÿ®ÿßŸÑŸá ÿ≥ŸàÿßŸÇŸäŸÜ ÿ®ÿ™ÿ®ÿ±ÿ¥ŸÖ ŸÖŸÅŸäÿ¥ ÿ≠ÿ™Ÿä ÿ±ŸÇŸÖ ŸÑŸÑÿ¥ŸÉÿßŸàŸä ...\n",
            "1        ÿÆÿØŸÖÿ© ÿßŸÑÿØŸÅÿπ ÿ∑ÿ±ŸäŸÇ ÿßŸÑŸÉŸä ŸÜÿ™ ÿ™ŸàŸÇŸÅÿ™ ÿπŸÜÿØŸä ÿßÿµÿ®ÿ≠ ŸÅŸÇÿ∑ ÿßŸÑ...\n",
            "2        ÿ™ÿ∑ÿ®ŸäŸÇ ÿ∫ÿ®Ÿä ÿ¨ÿßÿ±Ÿä ÿ≠ÿ∞ŸÅŸá ÿπÿßŸÖŸÑŸäŸÜ ÿßŸÉŸàÿßÿØ ÿÆÿµŸÖ ŸÜÿ≥ÿ™ÿÆÿØŸÖŸáÿß ...\n",
            "3        ŸÅÿπŸÑÿß ÿ™ÿ∑ÿ®ŸäŸÇ ŸÖŸÖÿ™ÿßÿ≤ ŸÅŸâ ÿßŸÖŸÉÿßŸÜŸäÿ© Ÿäÿ™Ÿäÿ≠ ŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑÿ™ÿ∑ÿ®...\n",
            "4                ÿ≥Ÿäÿ° ÿ¨ÿØÿß ÿßÿ≥ÿπÿßÿ± ÿ±ÿ≥ŸàŸÖ ÿßŸÑÿ™ŸàÿµŸäŸÑ ÿ™ŸÖÿ™ ŸÑŸÑŸàÿßŸÇÿπ ÿµŸÑŸá\n",
            "                               ...                        \n",
            "32031    ÿßŸÑÿ™ÿ∑ÿ®ŸäŸÇ ÿßÿµÿ®ÿ≠ ÿ≥Ÿäÿ° ŸÑŸÑÿ∫ÿßŸäŸá ŸÜŸÇŸàŸÖ ÿ®ÿ∑ŸÑÿ® Ÿäÿ™ŸÖ ŸàÿµŸàŸÑ ÿßŸÑÿ∑...\n",
            "32032                                                     \n",
            "32033                 ÿßŸÑÿ®ÿßŸÇŸá ÿ®ÿ™ÿÆŸÑÿµ Ÿàÿ®ÿ¥ÿ≠ŸÜ ŸÖÿ±ÿ™ŸäŸÜ ÿ®ÿßŸÇŸá ÿßÿ∂ÿßŸÅŸäŸá\n",
            "32034    ÿ™ÿ∑ÿ®ŸäŸÇ ŸÅÿßÿ¥ŸÑ ŸàÿµŸÑŸÜŸä ÿßŸÑÿ∑ŸÑÿ® ŸÜÿßŸÇÿµ ŸàŸÖÿ¥ ŸäŸÜŸÅÿπ ÿßÿπŸÖŸÑ ÿ≠ÿßÿ¨ÿ©...\n",
            "32035                                         ŸÑŸäÿ¥ ŸäŸÅÿ™ÿ≠ ŸÖÿπŸä\n",
            "Name: clean_text, Length: 32036, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install python-bidi\n",
        "%pip install langid\n",
        "%pip install keras_preprocessing\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QGcj6qiyNYcL",
        "outputId": "7f2e7b44-e11b-47ad-d632-cd3663ee335c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-bidi\n",
            "  Downloading python_bidi-0.4.2-py2.py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from python-bidi) (1.16.0)\n",
            "Installing collected packages: python-bidi\n",
            "Successfully installed python-bidi-0.4.2\n",
            "Collecting langid\n",
            "  Downloading langid-1.1.6.tar.gz (1.9 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from langid) (1.23.5)\n",
            "Building wheels for collected packages: langid\n",
            "  Building wheel for langid (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langid: filename=langid-1.1.6-py3-none-any.whl size=1941172 sha256=dab5bbe60f87ab15849d574d0a12d906840f13421583d39b1f7a178448151a3b\n",
            "  Stored in directory: /root/.cache/pip/wheels/23/c8/c6/eed80894918490a175677414d40bd7c851413bbe03d4856c3c\n",
            "Successfully built langid\n",
            "Installing collected packages: langid\n",
            "Successfully installed langid-1.1.6\n",
            "Collecting keras_preprocessing\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.23.5)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from keras_preprocessing) (1.16.0)\n",
            "Installing collected packages: keras_preprocessing\n",
            "Successfully installed keras_preprocessing-1.1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, Dense, Dropout, Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "import keras.preprocessing.sequence\n",
        "\n",
        "tokenizer = Tokenizer(num_words=7000)  # Adjust based on your actual vocabulary size\n",
        "class NN:\n",
        "    def __init__(self, df):\n",
        "        self.df = df\n",
        "\n",
        "    def start(self):\n",
        "      df = self.df\n",
        "\n",
        "      # Apply text cleaning function to 'review_description' and create 'clean_text' column\n",
        "      df['clean_text'] = df['review_description'].apply(clean_text)\n",
        "\n",
        "      mapping = {-1: 0, 0: 1, 1: 2}\n",
        "      df['rating'] = df['rating'].replace(mapping)\n",
        "\n",
        "      # Adjust the vocabulary size in the Tokenizer\n",
        "\n",
        "      tokenizer.fit_on_texts(df['clean_text'])\n",
        "\n",
        "      X = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "      X = keras.preprocessing.sequence.pad_sequences(X, maxlen=16)\n",
        "\n",
        "      model = self.build_lstm_model_with_embedding(input_shape=(X.shape[1],), output_units=3, vocab_size=7000)\n",
        "      model.summary()\n",
        "      model.fit(X, df['rating'], epochs=10, batch_size=32, validation_split=0.2)\n",
        "\n",
        "      self.model = model  # Save the model for later use\n",
        "\n",
        "    def build_lstm_model_with_embedding(self, input_shape, output_units=3, vocab_size=5000, embedding_dim=50):\n",
        "        model = Sequential()\n",
        "\n",
        "        # Adding an Embedding layer\n",
        "        model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_shape[0]))\n",
        "\n",
        "\n",
        "        # Adding an LSTM layer\n",
        "        model.add(LSTM(units=128, activation='relu', return_sequences=True))  # Set return_sequences=True for subsequent LSTM layers\n",
        "\n",
        "        # Adding a dropout layer\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # Adding another LSTM layer\n",
        "        model.add(LSTM(units=128, activation='relu'))\n",
        "\n",
        "        # Adding another dropout layer\n",
        "        model.add(Dropout(0.2))\n",
        "\n",
        "        # Output layer with softmax activation for classification\n",
        "        model.add(Dense(units=output_units, activation='softmax'))\n",
        "\n",
        "        # Compiling the model\n",
        "        model.compile(optimizer='adam',\n",
        "                      loss='sparse_categorical_crossentropy',\n",
        "                      metrics=['accuracy'])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def predict(self, X_test):\n",
        "        # Assuming X_test is a DataFrame with a column 'review_description'\n",
        "        X_test['clean_text'] = X_test['review_description'].apply(clean_text)\n",
        "\n",
        "        # Tokenize and pad the sequences\n",
        "        X_test_sequences = tokenizer.texts_to_sequences(X_test['clean_text'])\n",
        "        X_test_padded = keras.preprocessing.sequence.pad_sequences(X_test_sequences, maxlen=16)\n",
        "\n",
        "        # Make predictions\n",
        "        predictions = self.model.predict(X_test_padded)\n",
        "\n",
        "\n",
        "\n",
        "        return predictions"
      ],
      "metadata": {
        "id": "8oIV4bhPNPm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your data\n",
        "data = pd.read_excel(\"/content/train.xlsx\")\n",
        "data = pd.DataFrame(data, columns=['review_description', 'rating'])\n",
        "\n",
        "# Create NN object and train the model\n",
        "nn_model = NN(data)\n",
        "nn_model.start()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljzysF2TTJ3d",
        "outputId": "92a6c1e8-3bf1-4142-bf40-30de45bcc953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 16, 50)            350000    \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (None, 16, 128)           91648     \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16, 128)           0         \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (None, 128)               131584    \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 387       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 573619 (2.19 MB)\n",
            "Trainable params: 573619 (2.19 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "801/801 [==============================] - 33s 39ms/step - loss: 0.5678 - accuracy: 0.7846 - val_loss: 0.4785 - val_accuracy: 0.8237\n",
            "Epoch 2/10\n",
            "801/801 [==============================] - 31s 39ms/step - loss: 0.4301 - accuracy: 0.8496 - val_loss: 0.4886 - val_accuracy: 0.8198\n",
            "Epoch 3/10\n",
            "801/801 [==============================] - 31s 39ms/step - loss: 0.3733 - accuracy: 0.8649 - val_loss: 0.5476 - val_accuracy: 0.8198\n",
            "Epoch 4/10\n",
            "801/801 [==============================] - 31s 39ms/step - loss: 0.3275 - accuracy: 0.8831 - val_loss: 0.5520 - val_accuracy: 0.8121\n",
            "Epoch 5/10\n",
            "801/801 [==============================] - 32s 39ms/step - loss: 0.3040 - accuracy: 0.8921 - val_loss: 0.6892 - val_accuracy: 0.8118\n",
            "Epoch 6/10\n",
            "801/801 [==============================] - 32s 40ms/step - loss: 0.2708 - accuracy: 0.9036 - val_loss: 0.7551 - val_accuracy: 0.8051\n",
            "Epoch 7/10\n",
            "801/801 [==============================] - 32s 39ms/step - loss: 0.2468 - accuracy: 0.9111 - val_loss: 0.8521 - val_accuracy: 0.7954\n",
            "Epoch 8/10\n",
            "801/801 [==============================] - 33s 41ms/step - loss: 0.2368 - accuracy: 0.9152 - val_loss: 0.8655 - val_accuracy: 0.7965\n",
            "Epoch 9/10\n",
            "801/801 [==============================] - 32s 40ms/step - loss: 0.2167 - accuracy: 0.9218 - val_loss: 0.8427 - val_accuracy: 0.7962\n",
            "Epoch 10/10\n",
            "801/801 [==============================] - 32s 40ms/step - loss: 0.2010 - accuracy: 0.9272 - val_loss: 0.9859 - val_accuracy: 0.7945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the test data\n",
        "X_test = pd.read_csv(\"/content/test _no_label.csv\")\n",
        "\n",
        "# Make predictions\n",
        "predictions = nn_model.predict(X_test)\n",
        "predicted_labels = np.argmax(predictions,axis=1)\n",
        "# Define the mapping dictionary\n",
        "mapping = {0: -1, 1: 0, 2: 1}\n",
        "\n",
        "# Use the mapping dictionary to replace the values in the array\n",
        "mapped_data = np.array([mapping[value] for value in predicted_labels])\n",
        "\n",
        "# Print the mapped array\n",
        "print(mapped_data)"
      ],
      "metadata": {
        "id": "ZLzcJ4YKcLUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15abe0d8-c6c2-41f1-c6b4-9331ed3a621b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32/32 [==============================] - 1s 10ms/step\n",
            "[ 1  1 -1  1  1  1 -1 -1  1 -1  1  1  1 -1  0  1  1 -1 -1 -1  1  1  1  1\n",
            "  0 -1  1  1  1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1 -1  1 -1\n",
            "  1  1  1  1 -1 -1 -1  1 -1  0 -1 -1  1  1 -1  0  1  1 -1  1 -1 -1 -1  1\n",
            "  1 -1 -1 -1  1 -1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1  1\n",
            " -1 -1  1  0 -1 -1  1 -1  1  1 -1 -1  1  1  1  1  1  1  1  0  1  1  1  1\n",
            "  1  1  1  1  1  1  1 -1  1  1 -1 -1  1  1  1 -1  1  1  0  1  1 -1  1  1\n",
            "  1 -1 -1 -1  1  1  1  1  1  0 -1  1  1 -1  0  1  1  1  0  1 -1 -1 -1  1\n",
            "  1  1  1 -1  1  1  1  1  1  0  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1\n",
            "  1  1 -1  1 -1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1  0  1  1 -1 -1  1\n",
            "  1 -1  1  1  1 -1  1  1 -1  1 -1 -1  1  1  1  1  1  1  1  1 -1  1  1 -1\n",
            " -1 -1  1  1 -1  1  1 -1  1  1  1 -1  1 -1 -1  1 -1  1 -1  1  0  1  0  1\n",
            "  1  1 -1 -1 -1  1 -1  1 -1 -1  1 -1 -1  1  1  1  1  1  1 -1 -1  1  1  1\n",
            "  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
            "  1  1  1  1 -1 -1  1  0 -1 -1  1 -1  1  1  1  1  1 -1  1  0  1  1  1  1\n",
            "  0  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1 -1 -1  1  1  1  1  0  1 -1\n",
            "  1 -1  0 -1 -1  1  1 -1 -1 -1  1 -1  1 -1 -1  1  1  1 -1  1  1  1 -1 -1\n",
            "  1  1  1  1  1 -1  1  1  1 -1  1  1  1 -1 -1  1  1  1  1  1 -1  1 -1 -1\n",
            "  1  1  1  1 -1  1  1 -1 -1 -1  1 -1  1  1  1  1  1 -1  1  1 -1 -1 -1  1\n",
            "  1  1 -1  1  1 -1  1  1 -1  1  1  1  1  1 -1  1  1 -1  1  1 -1 -1  1  1\n",
            "  1  1 -1  1  1  1  1 -1  1  1  1 -1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
            "  1 -1  1  1  1  1  1  1  1  1 -1 -1  1 -1  0  1  1  1  1  1 -1 -1  1  1\n",
            "  1  1 -1 -1 -1 -1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1 -1  1\n",
            " -1  1 -1  1  1 -1 -1 -1  1  1  1 -1  1  1  1  1  1  1  1 -1  1 -1  1  1\n",
            "  1  1  1  1  1  1  1  1 -1 -1  1  1  1 -1 -1 -1  1  1 -1  1 -1  1  1 -1\n",
            "  1  1  1  1 -1  1  1  1  1  1  1  1  1 -1 -1  1  1 -1  1  1 -1  1  1  1\n",
            "  1 -1  1 -1  1  1  1 -1  1  1  1  1  1  1 -1  0  1  1  1  1  1  1  1 -1\n",
            " -1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1\n",
            "  1  1  1  0  1  1  1 -1 -1  1 -1  1 -1  1  1  1  1 -1 -1 -1  1 -1  1 -1\n",
            "  1 -1 -1  1  1 -1  1 -1  1  1 -1  1  1  1 -1  1  1  1  1  1 -1  1  1 -1\n",
            "  1  1 -1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1  0  0 -1\n",
            "  1  0  1 -1 -1  1  1 -1  1  1  1  1 -1  1 -1  1  1 -1  1  1 -1 -1  1  1\n",
            "  1  1 -1  1  1 -1  1 -1 -1  1  1 -1 -1  1  1 -1 -1  1  1  1 -1 -1 -1 -1\n",
            "  1  1  1  1  1  1 -1 -1 -1  1  0  1  1  1  1  1 -1  1  1  0  1  1  1 -1\n",
            "  1 -1  1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  0  1  1 -1  1 -1  1\n",
            " -1  1  1  1  0  1 -1  1  1  1  1  1  0 -1  1 -1  1  1 -1  1  1  1  1 -1\n",
            "  1  1  1  1  1 -1  1 -1  0  1  1  1 -1 -1 -1  1  1  0  1 -1 -1  1  1 -1\n",
            "  1  1  1  1  1  1  1  1 -1 -1  1  1 -1  0  1  1  1  1 -1  1  1  1  1  1\n",
            " -1  1  1  1  1  1 -1  1  1  1  1  1  1 -1  1  1  1  0 -1  1  0 -1  1 -1\n",
            "  1  1  1  1 -1  1  0 -1 -1  1  1  1  1  1  1 -1  1  1 -1  0 -1  1  1 -1\n",
            "  0  1  1  1  1  1 -1 -1 -1  1  1  1  0  1  1  1 -1 -1  1 -1 -1 -1 -1  1\n",
            "  1 -1 -1  1  1  1 -1 -1  1 -1 -1  1  1  1  1  1  1  1 -1  1  1  1 -1 -1\n",
            "  1 -1  0 -1  1  1  1  1  1  1  1  1 -1  1  1 -1]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "submission=pd.DataFrame()\n",
        "submission['ID']=X_test['ID']\n",
        "submission['rating']=mapped_data\n",
        "\n",
        "print(submission)\n",
        "submission.to_csv(\"neural_project5.csv\",index = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSII6opIWdQz",
        "outputId": "6c7d5d9b-514a-4091-d0e2-cd7f37838a7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       ID  rating\n",
            "0       1       1\n",
            "1       2       1\n",
            "2       3       1\n",
            "3       4       1\n",
            "4       5       1\n",
            "..    ...     ...\n",
            "995   996       1\n",
            "996   997      -1\n",
            "997   998       1\n",
            "998   999      -1\n",
            "999  1000      -1\n",
            "\n",
            "[1000 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Model**"
      ],
      "metadata": {
        "id": "53ew-r5My8w3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pandas as pd\n",
        "\n",
        "# Custom layer for multi-head self-attention\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super(MultiHeadSelfAttention, self).__init__()\n",
        "        # Dimensionality of the embedding and number of attention heads\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        # Dimension of each attention head\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Ensure embedding dimension is divisible by the number of heads\n",
        "        assert (\n",
        "            self.head_dim * num_heads == embed_dim\n",
        "        ), \"Embedding dimension needs to be divisible by the number of heads\"\n",
        "\n",
        "        # Linear transformations for query, key, value, and output\n",
        "        self.wq = layers.Dense(embed_dim)\n",
        "        self.wk = layers.Dense(embed_dim)\n",
        "        self.wv = layers.Dense(embed_dim)\n",
        "        self.dense = layers.Dense(embed_dim)\n",
        "\n",
        "    # Helper function to split heads of the input tensor\n",
        "    def split_heads(self, x, batch_size):\n",
        "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.head_dim))\n",
        "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # Forward pass of the multi-head self-attention layer\n",
        "    def call(self, v, k, q, mask):\n",
        "        batch_size = tf.shape(q)[0]\n",
        "\n",
        "        # Linear transformations for query, key, and value\n",
        "        q = self.wq(q)\n",
        "        k = self.wk(k)\n",
        "        v = self.wv(v)\n",
        "\n",
        "        # Splitting into multiple heads\n",
        "        q = self.split_heads(q, batch_size)\n",
        "        k = self.split_heads(k, batch_size)\n",
        "        v = self.split_heads(v, batch_size)\n",
        "\n",
        "        # Scaled dot-product attention mechanism\n",
        "        scaled_attention, attention_weights = self.scaled_dot_product_attention(\n",
        "            q, k, v, mask\n",
        "        )\n",
        "\n",
        "        # Rearrange and concatenate attention heads\n",
        "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.embed_dim))\n",
        "\n",
        "        # Linear transformation for the output\n",
        "        output = self.dense(concat_attention)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "    # Scaled dot-product attention mechanism\n",
        "    def scaled_dot_product_attention(self, q, k, v, mask):\n",
        "        matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
        "        dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
        "        scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
        "\n",
        "        # Apply mask to prevent attending to padded positions\n",
        "        if mask is not None:\n",
        "            scaled_attention_logits += mask * -1e9\n",
        "\n",
        "        # Softmax activation to get attention weights\n",
        "        attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
        "        # Weighted sum using attention weights\n",
        "        output = tf.matmul(attention_weights, v)\n",
        "\n",
        "        return output, attention_weights\n",
        "\n",
        "\n",
        "# Custom layer for a Transformer block\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        # Multi-head self-attention layer\n",
        "        self.att = MultiHeadSelfAttention(embed_dim, num_heads)\n",
        "        # Feedforward neural network\n",
        "        self.ffn = tf.keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim)]\n",
        "        )\n",
        "        # Layer normalization for both attention and feedforward output\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        # Dropout layers for regularization\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    # Forward pass of the Transformer block\n",
        "    def call(self, inputs, training=True):\n",
        "        # Multi-head self-attention layer\n",
        "        attn_output, _ = self.att(inputs, inputs, inputs, mask=None)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "\n",
        "        # Feedforward neural network\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "\n",
        "        # Residual connection and layer normalization\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "\n",
        "# Custom layer for token and position embedding\n",
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        # Embedding layer for tokens\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        # Embedding layer for positions\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    # Forward pass of the token and position embedding layer\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        # Create positions tensor\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        # Token embedding and addition with position embeddings\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions\n",
        "\n",
        "\n",
        "# Function to create the entire transformer model\n",
        "def create_transformer_model(maxlen, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes):\n",
        "    inputs = layers.Input(shape=(maxlen,))\n",
        "    # Token and position embedding layer\n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
        "    x = embedding_layer(inputs)\n",
        "\n",
        "    # Stack multiple Transformer blocks\n",
        "    for _ in range(num_blocks):\n",
        "        x = TransformerBlock(embed_dim, num_heads, ff_dim)(x)\n",
        "\n",
        "    # Global average pooling to reduce spatial dimensions\n",
        "    x = layers.GlobalAveragePooling1D()(x)\n",
        "    # Dropout layer for regularization\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    # Dense layer with ReLU activation for additional non-linearity\n",
        "    x = layers.Dense(20, activation=\"relu\")(x)\n",
        "    # Additional dropout layer\n",
        "    x = layers.Dropout(0.1)(x)\n",
        "    # Output layer with softmax activation for classification\n",
        "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
        "\n",
        "    return tf.keras.Model(inputs=inputs, outputs=outputs)\n"
      ],
      "metadata": {
        "id": "nj65c758ytaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Training**"
      ],
      "metadata": {
        "id": "qiLSSG_Sy4Af"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data from an Excel file into a pandas DataFrame\n",
        "df = pd.read_excel(\"train.xlsx\")\n",
        "\n",
        "# Apply a text cleaning function to the 'review_description' column\n",
        "df['review_description'] = df['review_description'].apply(lambda x: clean_text(x))\n",
        "\n",
        "# Tokenize and pad text data\n",
        "max_len = 100\n",
        "\n",
        "# Initialize a tokenizer and fit it on the preprocessed text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['review_description'])\n",
        "\n",
        "# Convert text sequences to numerical sequences and pad to a fixed length\n",
        "sequences = tokenizer.texts_to_sequences(df['review_description'])\n",
        "X = pad_sequences(sequences, maxlen=max_len)\n",
        "\n",
        "# Extract labels and add 1 to each label to make them 0, 1, 2\n",
        "y = df['rating'].values + 1\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Calculate the vocabulary size for the tokenizer\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Model configuration parameters\n",
        "embed_dim = 32\n",
        "num_heads = 2\n",
        "ff_dim = 32\n",
        "num_blocks = 3\n",
        "num_classes = 3\n",
        "\n",
        "# Build and compile the transformer model\n",
        "model = create_transformer_model(max_len, vocab_size, embed_dim, num_heads, ff_dim, num_blocks, num_classes)\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "# Training configuration parameters\n",
        "batch_size = 32\n",
        "epochs = 5\n",
        "\n",
        "# Train the model on the training set, with early stopping to prevent overfitting\n",
        "model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_test, y_test), callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Evaluate the model on the testing set\n",
        "loss, accuracy = model.evaluate(X_test, y_test)\n",
        "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")\n",
        "\n",
        "# Save the trained model to a file\n",
        "model.save(\"transformer_model.keras\")\n"
      ],
      "metadata": {
        "id": "zuHeOrbOy2a2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Transformer Testing**"
      ],
      "metadata": {
        "id": "_nyg6mCTy_jP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the testing dataset\n",
        "test_df = pd.read_csv('test _no_label.csv')\n",
        "\n",
        "# Tokenize and pad the text data in the new DataFrame\n",
        "new_sequences = tokenizer.texts_to_sequences(test_df['review_description'])\n",
        "new_X = pad_sequences(new_sequences, maxlen=max_len)\n",
        "\n",
        "# Make predictions using the trained model\n",
        "predictions = model.predict(new_X)\n",
        "\n",
        "# Convert the predicted probabilities to class labels\n",
        "predicted_labels = predictions.argmax(axis=1) - 1  # Subtract 1 to convert back to -1, 0, 1\n",
        "\n",
        "# Add the predicted labels to the new DataFrame\n",
        "test_df['rating'] = predicted_labels\n",
        "test_df = test_df[['ID', 'rating']]\n",
        "\n",
        "test_df.to_csv('Results.csv', index = False)"
      ],
      "metadata": {
        "id": "ZR-q3TYYzChA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pfqLAd_IwhXW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}